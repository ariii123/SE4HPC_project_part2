name: CI Pipeline

on:
  push:
    branches:
      - main

env:
  BUILD_TYPE: Release

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v2
      with:
        submodules: true  # Clona anche i submoduli


    - name: Cache singularity binaries
      id: cache-singularity
      uses: actions/cache@v2
      with:
        path: /usr/local/bin/singularity
        key: ${{ runner.os }}-singularity-${{ hashFiles('**/singularity-version.txt') }}
        restore_keys: |
          ${{ runner.os }}-singularity-

    - name: Cache singularity container image
      id: cache-image
      uses: actions/cache@v2
      with:
        path: singularity/cont.sif
        key: ${{ runner.os }}-singularity-image-
        restore_keys: ${{ runner.os }}-singularity-image- #${{ hashFiles('singularity/container.def') }}

    - name: Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y $(cat apt-packages.txt)
      shell: bash

#    - name: Update Submodules
#      run: |
#        git submodule update --init --recursive
#      shell: bash
#
#    - name: Configure CMake
#      run: cmake -S . -B build
#
#    - name: Build
#      run: cmake --build build
#
#    - name: Run Tests
#      run: ctest --test-dir build
      

#    - name: Setup Singularity
#      #if: steps.cache-singularity.outputs.cache-hit != 'true'
#      uses: eWaterCycle/setup-singularity@v7
#      with:
#        singularity-version: 3.8.3
#      
#    - name: Verify singularity version
#      run: singularity --version

    - name: Build Singularity Container
      if: steps.cache-image.outputs.cache-hit != 'true'
      run: singularity build --fakeroot singularity/cont.sif singularity/container.def

    - name: Send image, job and data to the cluster
      run: sshpass -p '${{ secrets.CINECA_PASSWORD }}' scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null singularity/cont.sif job.sh matrix* ${{ secrets.CINECA_USER_NAME}}@login.g100.cineca.it:/g100/home/usertrain/${{ secrets.CINECA_USER_NAME }}/


    - name: Execute job on the cluster
      run: sshpass -p '${{ secrets.CINECA_PASSWORD }}' ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.CINECA_USER_NAME}}@login.g100.cineca.it "sh ./job.sh"

    - name: Copying the data back
      run: sshpass -p '${{ secrets.CINECA_PASSWORD }}' scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.CINECA_USER_NAME}}@login.g100.cineca.it:/g100/home/usertrain/${{ secrets.CINECA_USER_NAME }}/std* ./ && cat std*


    - name: Cleaning the used directory on the cluster
      run: sshpass -p '${{ secrets.CINECA_PASSWORD }}' ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.CINECA_USER_NAME}}@login.g100.cineca.it "rm job.sh cont.sif *.txt"

  

    

